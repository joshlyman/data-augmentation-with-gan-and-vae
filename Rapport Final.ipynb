{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN-Generated Samples to Offset Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vincent Fortin (11249631) | \n",
    "Nicolas Gervais (11263889)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Intro  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Class Imbalance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with imbalanced classes represents a challenge that most machine learning practitioners will face. Indeed, many learning algorithms are suited for balanced datasets, such as Support Vector Machines (SVM), decision trees, and logistic regression [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0020025513005124). When combined with a limited number of training instances, imbalanced classes can result in poorly trained models. Having few instances from which to learn, algorithms may have a limited ability to generalize, and therefore suffer from poor performance on unseen data. These problems have most frequently sparked research in the fields of neurocomputing, knowledge-based systems, but also in image recognition [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0957417416307175?via%3Dihub). Many "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various strategies have been suggested to negate the effects of class imbalance, which typically fall into three categories, oversampling, undersampling, and hybrid methods [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0957417416307175?via%3Dihub). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the category of oversampling, probably the most popular strategy is to use the Synthetic Minority Over-sampling Technique (SMOTE). As its name suggests, SMOTE is an oversampling method, which works by creating synthetic samples from the minor class instead of creating copies [$^{ref}$](https://jair.org/index.php/jair/article/view/10302). The algorithm selects two or more similar instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances. Many rules have been put forward to weigh minority instances differently. A suggestion has been to cluster minority instances using a semi-unsupervised hierarchical clustering approach to determine the size to oversample each sub-cluster using its classification complexity and cross-validation. Then, the minority instances are oversampled depending on their Euclidean distance to the majority class. [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0957417415007356) <font color='red'>_this excerpt was copy pasted_</font>. Minority instances can also be weighted according to their distance to the majority class [$^{ref}$](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0180830). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar family of strategies is the undersampling of the majority class. Interestingly, the most effective method of this kind is to delete random samples until the size of the majority and minority classes match [$^{ref}$](https://link.springer.com/chapter/10.1007%2F978-3-642-02326-2_9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, hybrid methods are a mixture of the two aforementioned strategies. A recent meta-analysis from Haixiang and colleagues (2017) offers more details of the latest developments of research on imbalanced datasets [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0957417416307175?via%3Dihub)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2 What has been done in AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counteracting the effects of class imbalance in image recognition tasks add another layer of difficulty. Yet, some methods have been suggested. By matching pairs of images (taking the mean of every pixel), accuracy was improved on the CIFAR-10, compared to the generic dataset [$^{ref}$](https://arxiv.org/abs/1801.02929). In a similar fashion, state-of-the-art results on the CIFAR-10 and ImageNet have been achieved using translation, rotation, or shearing of different magnitudes [$^{ref}$](https://arxiv.org/abs/1805.09501). Another method to provide more training samples was to cut the \"main\" component of the image, and paste it on different backgrounds [$^{ref}$](http://openaccess.thecvf.com/content_iccv_2017/html/Dwibedi_Cut_Paste_and_ICCV_2017_paper.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other possibilities to generate more training samples include generative adversarial networks (GAN) and variational auto-encoders (VAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.1 GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs are neural networks defined by a _generator_ and a _discriminator_. The former generates increasingly realistic samples, and the discriminator determines if the samples looks \"real\" or \"fake\". The term adversarial refers to the competitive nature of the interaction between generation and discrimination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.2 How GAN has been used to counter class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researchers have used GANs to generate new minority samples [$^{ref}$](https://arxiv.org/abs/1807.04585). A balanced GAN (BAGAN) was designed with both the majority and minority class to learn useful features. The authors found that the pictures generated were of higher quality than simply using the minority class [$^{ref}$](https://arxiv.org/abs/1803.09655). However, the authors did not test if this resulted in a more accurate classifier. Similar to our research question, researchers have used GANs to generate instances of multiple classes, and found increased CNN accuracy [$^{ref}$](https://www.sciencedirect.com/science/article/pii/S0925231219309257?dgcid=rss_sd_all), over and above generic oversampling. Importantly, intra-class heterogeneity must be captured by the GAN, to provide new boundaries to the parameter space. With this concern in mind, Huang and colleagues (2019) improved classification accuracy with their actor-critic GAN (AC-GAN) [$^{ref}$](https://ieeexplore.ieee.org/document/8784774)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Our experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Our experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Our task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of GANs and VAEs, we will generate a multitude of samples for a minority class, and determine if these generated samples improve a classifier. A simple face recognition task will be assessed: to determine the sex of the person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used will be the UTK Face Dataset [$^{ref}$](http://aicip.eecs.utk.edu/wiki/UTKFace), which consists of over 20,000 face images with annotations of age, gender, and ethnicity. Only the pictures labeled as between 20 and 100 years old will be included. 8,000 samples will be kept for both the male and female categories. Next, the female class will be reduced to 10% of its original size, in order to weaken the classifier. The picture size is 60x60 in grayscale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Our benchmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we downsample women until we get 70% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Our metrics / CNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier will be a convolutional neural network (CNN) <font color='orange'>(describe shortly)</font>. CNNs are neural networks with at least one convolutional layer, which serves as a feature detector. See the following figure for the exact architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here plot keras pydot model plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier will be trained with both the _complete_ classes, and this performance will be established as the original baseline benchmark. Additionally, the CNN will be trained with the reduced female class, and it will be set as the lower bound classification performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the experiment will undergo various levels of imbalance, classification performance will be measured with the area under the curve (AUC). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, five models will be used to generate samples. All models contain interesting particularities for the task at hand, which are promising in unique ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.1 Variational Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variational auto-encoder (VAE) consists of an encoder, a decoder, and a loss function. The encoder transforms its input in a hidden representation, also called its latent representation space. This space is much less than the input dimensions. This is typically referred to as a ‘bottleneck’ because the encoder must learn an efficient compression of the data into this lower-dimensional space. The lower-dimensional space is stochastic: the encoder outputs parameters as a Gaussian probability density. We can sample from this distribution to get noisy values of the representations zz. <font color='red'> _this is copy pasted_ </font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is another neural net. Its input is the representation output by the encoder, it outputs the parameters to the probability distribution of the data. By re-expanding the representation, we can determine how much information is lost with the reconstruction log-likelihood loss. This measure tells us how effectively the decoder has learned to reconstruct an input image xx given its latent representation zz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function of the variational autoencoder is the negative log-likelihood with a regularizer. If the decoder’s output does not reconstruct the data well, statistically we say that the decoder parameterizes a likelihood distribution that does not place much probability mass on the true data. The second term is a regularizer that we throw in (we’ll see how it’s derived later). This is the Kullback-Leibler divergence.  between the encoder’s distribution q_\\theta(z\\mid x)q\n",
    "​θ\n",
    "​​ (z∣x) and p(z)p(z). This divergence measures how much information is lost (in units of nats) when using qq to represent pp. It is one measure of how close qq is to pp.\n",
    "\n",
    "In the variational autoencoder, pp is specified as a standard Normal distribution with mean zero and variance one, or p(z) = Normal(0,1)p(z)=Normal(0,1). If the encoder outputs representations zz that are different than those from a standard normal distribution, it will receive a penalty in the loss. This regularizer term means ‘keep the representations zz of each digit sufficiently diverse’. If we didn’t include the regularizer, the encoder could learn to cheat and give each datapoint a representation in a different region of Euclidean space. This is bad, because then two images of the same number (say a 2 written by different people, 2_{alice}2\n",
    "​alice\n",
    "​​  and 2_{bob}2\n",
    "​bob\n",
    "​​ ) could end up with very different representations z_{alice}, z_{bob}z\n",
    "​alice\n",
    "​​ ,z\n",
    "​bob\n",
    "​​ . We want the representation space of zz to be meaningful, so we penalize this behavior. This has the effect of keeping similar numbers’ representations close together (e.g. so the representations of the digit two {z_{alice}, z_{bob}, z_{ali}}z\n",
    "​alice\n",
    "​​ ,z\n",
    "​bob\n",
    "​​ ,z\n",
    "​ali\n",
    "​​  remain sufficiently close)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.2 Adversarial Auto-Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.3 Softmax GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.4 Wasserstein GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.5 Deep Convolutional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Our results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Alternative results (CNN trained on original data, test is generated data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
